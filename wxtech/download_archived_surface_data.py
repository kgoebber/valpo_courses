"""
download_archived_surface_data.py

Original Script from Darryl Herzmann at Iowa State University
Example script that scrapes data from the IEM ASOS download service

Modifications:

This script has been modified to include incorporation of MetPy to calculate
and add additional variables that are obtained from the Iowa State archive. The
script reads the data from the archive using Pandas, calculates new variables 
using archive variables and calculations from MetPy, and saves the final file as
a CSV format file. The new variables generated by this script match those created
from reading METAR files with MetPy.

Added variables include:
 - Cloud Cover in Oktas from the skyc1 archive variable
 - Air Pressure at Sea Level from the alti archive variable using MetPy calculation
 - Eastward Wind from the drct and sknt archive variables
 - Northward Wind from the drct and sknt archive variables
 
Script will ask for a beginning and end date for the desired surface observation.
The inputs will be asked for successively and need to be in a YYYYMMDD format.

Last Modified: 11 March 2021 by KHG
"""
from __future__ import print_function
from io import StringIO
import json
import time
import datetime

import metpy.calc as mpcalc
from metpy.plots import declarative, wx_code_to_numeric
from metpy.units import units
import pandas as pd

def get_cloud_cover(code):
    '''Function to covert text cloud cover into MetPy usable numeric notation.'''
    if 'OVC' in code:
        return 1
    elif 'BKN' in code:
        return 6
    elif 'SCT' in code:
        return 4
    elif 'FEW' in code:
        return 2
    else:
        return 0

# Python 2 and 3: alternative 4
try:
    from urllib.request import urlopen
except ImportError:
    from urllib2 import urlopen

# Number of attempts to download data
MAX_ATTEMPTS = 6
# HTTPS here can be problematic for installs that don't have Lets Encrypt CA
SERVICE = "http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?"


def download_data(uri):
    """Fetch the data from the IEM
    The IEM download service has some protections in place to keep the number
    of inbound requests in check.  This function implements an exponential
    backoff to keep individual downloads from erroring.
    Args:
      uri (string): URL to fetch
    Returns:
      string data
    """
    attempt = 0
    while attempt < MAX_ATTEMPTS:
        try:
            data = StringIO(urlopen(uri, timeout=300).read().decode("utf-8", 'backslashreplace'))
            #if data is not None and not data.startswith("ERROR"):
            #    return data
            return data
        except Exception as exp:
            print("download_data(%s) failed with %s" % (uri, exp))
            time.sleep(5)
        attempt += 1

    print("Exhausted attempts to download, returning empty data")
    return ""


def get_stations_from_filelist(filename):
    """Build a listing of stations from a simple file listing the stations.
    The file should simply have one station per line.
    """
    stations = []
    for line in open(filename):
        stations.append(line.strip())
    return stations


def get_stations_from_networks():
    """Build a station list by using a bunch of IEM networks."""
    stations = []
    states = """AK AL AR AZ CA CO CT DE FL GA HI IA ID IL IN KS KY LA MA MD ME
     MI MN MO MS MT NC ND NE NH NJ NM NV NY OH OK OR PA RI SC SD TN TX UT VA VT
     WA WI WV WY"""
    # IEM quirk to have Iowa AWOS sites in its own labeled network
    networks = ["AWOS"]
    for state in states.split():
        networks.append("%s_ASOS" % (state,))

    for network in networks:
        # Get metadata
        uri = (
            "https://mesonet.agron.iastate.edu/geojson/network/%s.geojson"
        ) % (network,)
        data = urlopen(uri)
        jdict = json.load(data)
        for site in jdict["features"]:
            stations.append(site["properties"]["sid"])
    return stations


def download_alldata(startts=datetime.datetime(2012, 8, 1), endts=datetime.datetime(2012, 9, 1)):
    """An alternative method that fetches all available data.
    Service supports up to 24 hours worth of data at a time."""
    # timestamps in UTC to request data for
    #startts = datetime.datetime(2012, 8, 1)
    #endts = datetime.datetime(2012, 9, 1)
    interval = datetime.timedelta(hours=24)

    service = SERVICE + "data=all&tz=Etc/UTC&format=comma&latlon=yes&elev=yes&trace=0.0001&"

    now = startts
    while now < endts:
        thisurl = service
        thisurl += now.strftime("year1=%Y&month1=%m&day1=%d&")
        thisurl += (now + interval).strftime("year2=%Y&month2=%m&day2=%d&")
        print("Downloading: %s" % (now,))
        data = download_data(thisurl)
        df = pd.read_csv(data, skiprows=5, header=0,
                         parse_dates=['valid'], na_values=['M'],
                         low_memory=False)
        # Convert wx codes to numeric values for plotting purposes
        df['current_wx1_symbol'] = wx_code_to_numeric(df['wxcodes'].fillna(''))
        # Convert cloud cover strings to numeric values for plotting purposes
        df['cloud_cover'] = df.skyc1.fillna('').apply(get_cloud_cover)
        # Compute the u (eastward_wind) and v(northward_wind) components
        df['eastward_wind'], df['northward_wind'] = mpcalc.wind_components(df.sknt.values * units.knots,
                                                                           df.drct.values * units.degree)
        # Compute the MSLP from the Altimeter reading using MetPy calculation
        df['air_pressure_at_sea_level'] = mpcalc.altimeter_to_sea_level_pressure(df.alti.values * units.inHg,
                                                                                 df.elevation.values * units.meters,
                                                                                 df.tmpf.values * units.degF).to('hPa')
        # Save CSV version of archive and new variables
        df.to_csv(f'surface_data_{now:%Y%m%d}.csv', index=False)
        #outfn = "surface_data_%s.txt" % (now.strftime("%Y%m%d"),)
        #with open(outfn, "w") as fh:
        #    fh.write(data)
        now += interval


def main():
    """Our main method"""
    # timestamps in UTC to request data for
    startts = datetime.datetime(2012, 8, 1)
    endts = datetime.datetime(2012, 9, 1)

    service = SERVICE + "data=all&tz=Etc/UTC&format=comma&latlon=yes&"

    service += startts.strftime("year1=%Y&month1=%m&day1=%d&")
    service += endts.strftime("year2=%Y&month2=%m&day2=%d&")

    # Two examples of how to specify a list of stations
    stations = get_stations_from_networks()
    # stations = get_stations_from_filelist("mystations.txt")
    for station in stations:
        uri = "%s&station=%s" % (service, station)
        print("Downloading: %s" % (station,))
        data = download_data(uri)
        outfn = "surface_data_%s_%s_%s.csv" % (
            station,
            startts.strftime("%Y%m%d%H%M"),
            endts.strftime("%Y%m%d%H%M"),
        )
        out = open(outfn, "w")
        out.write(data)
        out.close()


if __name__ == "__main__":
    start_date = input("Specify the start date in YYYYMMDD format (e.g., 20120801): ")
    end_date = input("Specify the end date in YYYYMMDD format (e.g., 20120901): ")
    
    startts = datetime.datetime.strptime(start_date, '%Y%m%d')
    endts = datetime.datetime.strptime(end_date, '%Y%m%d') + datetime.timedelta(days=1)

    download_alldata(startts, endts)
